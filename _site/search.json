[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Thoughts and Scribbles",
    "section": "",
    "text": "Anomaly And Outlier Detection In Machine Learning\n\n\n\nclass-project\n\n\nmachine-learning\n\n\nanomaly-detection\n\n\noutlier-detection\n\n\n\nAn introduction to basic methodology in anomaly and outlier detection\n\n\n\nTurbasu Chatterjee\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Bengali MNIST using a Hybrid Quantum Neural Network\n\n\n\nclass-project\n\n\nmachine-learning\n\n\nclassification\n\n\nquantum-computing\n\n\n\nClassifying using quantum machine learning.\n\n\n\nTurbasu Chatterjee\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\n\n\nclass-project\n\n\nmachine-learning\n\n\nclustering\n\n\n\nA deep dive into the DBSCAN algorithm\n\n\n\nTurbasu Chatterjee\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding The Chernoff Bound In The Context Of Machine Learning\n\n\n\nclass-project\n\n\nmachine-learning\n\n\nprobability-theory\n\n\n\nModelling uncertainty using concentration bounds in the context of machine learning.\n\n\n\nTurbasu Chatterjee\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLinear and Non-Linear Regression: A Performance Comparison\n\n\n\nclass-project\n\n\nmachine-learning\n\n\nregression\n\n\n\nAn introduction and performance comparison of the linear and non-linear regression techniques.\n\n\n\nTurbasu Chatterjee\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/probability/Concentration_Bound.html",
    "href": "posts/probability/Concentration_Bound.html",
    "title": "Understanding The Chernoff Bound In The Context Of Machine Learning",
    "section": "",
    "text": "Modelling uncertainty is one of the prime objectives of machine learning. In this blog post we will take a deep dive into methods to model this uncertainty using concentration inequalities, specifically the Chernoff bound and the Hoeffding bound. Further, we shall use a simple example to illustrate how good these bounds are at what they do."
  },
  {
    "objectID": "posts/probability/Concentration_Bound.html#theoretical-foundations",
    "href": "posts/probability/Concentration_Bound.html#theoretical-foundations",
    "title": "Understanding The Chernoff Bound In The Context Of Machine Learning",
    "section": "Theoretical Foundations",
    "text": "Theoretical Foundations\nThe Chernoff inequality relies on moment generating functions (MGFs), which provides information on the distribution of random variables. The MGF of a random variable X is defined as:\n\\[\\begin{align}\nM_x(t) &= \\mathbb{E}[e^{tX}] \\\\\n        &= 1 + t \\mathbb{E}[X] + \\frac{t^2 \\mathbb{E}[X^2]}{2!} + \\frac{t^3 \\mathbb{E}[X^3]}{3!} + \\dots \\\\\n        &= 1 + tm_1 + \\frac{tm_2}{2!} + \\frac{tm_3}{3!} + \\dots\n\\end{align}\\]\nwhere, $m_1, m_2, $ are the first moment, second moment and so on, \\(X\\) is a random variable and \\(\\mathbb{E}[X]\\) is the expectation of that random variable.\nWe shall define a threshold, \\(a\\) as a positive real number for which we want to find the probability \\(\\Pr(X \\geq a)\\).\nFor positive \\(t\\), this gives a bound on the right tail of \\(X\\) in terms of its MGF. This is given by:\n\\[\\begin{equation}\n\\Pr(X \\geq a) = \\Pr(e^{tX} \\geq e^{ta}) \\leq M(t)e^{-ta} \\quad \\forall t &gt; 0.\n\\end{equation}\\]\nNow since this bound holds for for every \\(t&gt;0\\), we can take the infimum as follows:\n\\[\\begin{equation}\n\\Pr(X \\geq a) \\leq \\inf \\limits_{t &gt; 0} M(t) e^{-ta}.\n\\end{equation}\\]\nWe can obtain a bound on the right tail by performing similar analyses using \\(t &lt; 0\\).\n\\[\\begin{equation}\n\\Pr(X \\geq a) = \\Pr(e^{tX} \\geq e^{ta}) \\leq M(t)e^{-ta} \\quad \\forall t&lt;0.\n\\end{equation}\\]\nAgain, taking the infimum, we have\n\\[\\begin{equation}\n\\Pr(X \\geq a) \\leq \\inf \\limits_{t &lt; 0} M(t) e^{-ta}.\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/probability/Concentration_Bound.html#an-example-using-coin-flipping",
    "href": "posts/probability/Concentration_Bound.html#an-example-using-coin-flipping",
    "title": "Understanding The Chernoff Bound In The Context Of Machine Learning",
    "section": "An Example Using Coin Flipping",
    "text": "An Example Using Coin Flipping\nIn this scenario, we shall use the idea of simulating tossing biased coins. Lets begin this by calling our imports.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNow we shall generate our dataset using three biased coins. We shall do this by constructing a function that takes in the number of coin flips and the bias of the coin. Next we will visualize how the cumulative head probability changes over time, showing the law of large numbers in action.\n\nnp.random.seed(42)\nnum_tosses = 1000\ncoin_biases = [0.2, 0.5, 0.8]\n\ndef simulate_coin_toss(bias, num_tosses):\n    return np.random.choice([0, 1], size=num_tosses, p=[1 - bias, bias])\n\nfor bias in coin_biases:\n    tosses = simulate_coin_toss(bias, num_tosses)\n    cumulative_heads = np.cumsum(tosses) / np.arange(1, num_tosses + 1)\n    plt.plot(cumulative_heads, label=f'Bias = {bias}')\n\nplt.title('Cumulative Head Probability in Coin Tosses')\nplt.xlabel('Number of Tosses')\nplt.ylabel('Cumulative Head Probability')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\nNow we shall try to estimate how the cumulative probability of getting a head shifts deviates significantly from the true bias. Here, we shall employ the Chernoff bound that calculates the probability of the cumulative head probability deviating significantly from the true bias.\nIn order to do this, we emply a function that calculates the Chernoff bound as below:\n\ndef chernoff_bound(epsilon, N):\n    return 2 * np.exp(-2 * N * epsilon**2)\n\nNow we shall set our value of \\(\\varepsilon\\) which will serve as the error threshold. We plot a visualization to show how the Chernoff Inequality bounds (in blue) change as the number of coin tosses increases. The red dashed line represents the chosen error threshold \\(\\varepsilon\\), and we can observe how the bounds tighten with more data.\n\nepsilon = 0.1\n\n# Visualize Chernoff bounds for different biases\nplt.figure(figsize=(12, 6))\n\nfor bias in coin_biases:\n    bounds = [chernoff_bound(epsilon, n) for n in range(1, num_tosses + 1)]\n    plt.plot(bounds, label=f'Bias = {bias}')\n\nplt.axhline(y=epsilon, color='r', linestyle='--', label=f'epsilon = {epsilon}')\nplt.title('Chernoff Inequality Bounds')\nplt.xlabel('Number of Tosses')\nplt.ylabel('Chernoff Bound')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\nBy applying the Chernoff bound to practical examples, such as simulating biased coin tosses, and using visualizations, we can gain a deeper understanding of how this inequality works and how it allows us to set confidence bounds on real-world data.\nWe shall see how the Chernoff bound how the Chernoff bound plays an important role in machine learning research in the subsequent section."
  },
  {
    "objectID": "posts/probability/Concentration_Bound.html#using-the-chernoff-bound-in-machine-learning",
    "href": "posts/probability/Concentration_Bound.html#using-the-chernoff-bound-in-machine-learning",
    "title": "Understanding The Chernoff Bound In The Context Of Machine Learning",
    "section": "Using The Chernoff Bound in Machine Learning",
    "text": "Using The Chernoff Bound in Machine Learning\nSuppose the task at hand is to build and understand a credit card fraud detection model. We shall deem that credit card fraud detection is a rare event and we want to be able to stop misclassifying such events using the Chernoff bound.\nIn order to do this, we shall generate our own dataset for this example, such that we have two classes: “Genuine” and “Fraudulent” transactions. In order to signify the rarity of fraudulent events, we shall grossly limit the representation of these events by just 5% of the total data.\n\n# Calling all imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n# Generate synthetic data\nnp.random.seed(42)\n\ngenuine_transactions = np.random.normal(250, 20, 950)\nfraudulent_transactions = np.random.normal(250, 50, 50)\n\n\nWe will now create a label by assigning a 0 to the 950 real transactions and a 1 to the 50 fraudulent transactions. We then split the training and the testing data by using 20% of the data generated for verification and testing. We than use a Random Forest Classifier to train and classify our model. Then we evaluate our classification model use an accuracy metric.\n\n# Create labels\nlabels = [0] * 950 + [1] * 50\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(np.concatenate([genuine_transactions, fraudulent_transactions]), labels, test_size=0.2, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train.reshape(-1, 1), y_train)\ny_pred = clf.predict(X_test.reshape(-1, 1))\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Model Accuracy: {accuracy:.2%}\")\n\nModel Accuracy: 94.50%\n\n\nThe model accuracy comes out to 94.5%. However, there is more than what meets the eye. In order to see this, we must first visualize the dataset that we have generated.\n\nplt.figure(figsize=(10, 6))\nplt.hist(genuine_transactions, bins=30, alpha=0.5, label='Genuine Transactions', color='blue')\nplt.hist(fraudulent_transactions, bins=10, alpha=0.5, label='Fraudulent Transactions', color='red')\nplt.xlabel('Transaction Amount')\nplt.ylabel('Frequency')\nplt.title('Distribution of Genuine and Fraudulent Transactions')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\nNow we shall apply the Chernoff bound to estimate the probability of our Random Forest Classifier misclassifying a fraudulent transaction as a genuine one.\n\ndef chernoff_bound(epsilon, n):\n    return np.exp(-n * epsilon**2 / 2)\n\nepsilon = 0.05  # Desired error rate\nn_fraudulent = len(fraudulent_transactions)\nchernoff_probability = chernoff_bound(epsilon, n_fraudulent)\n\nprint(f\"Chernoff Probability: {chernoff_probability:.2%}\")\n\nChernoff Probability: 93.94%\n\n\nThe Chernoff probability i.e. the probability of datapoints being misclassified in our little experiment comes out to around 94%, which should be alarming, despite what our model accuracy metric tells us.\nIn order to calculate how the Chernoff probability changes as sample size increases, we have the following piece of code:\n\nsample_sizes = np.arange(1, len(fraudulent_transactions) + 1)\nchernoff_probabilities = [chernoff_bound(epsilon, n) for n in sample_sizes]\n\nplt.figure(figsize=(10, 6))\nplt.plot(sample_sizes, chernoff_probabilities, marker='o', linestyle='-', color='green')\nplt.xlabel('Sample Size')\nplt.ylabel('Chernoff Probability')\nplt.title('Chernoff Probability vs. Sample Size')\nplt.grid(True)\nplt.show()\n\n\n\n\nFinally we plot how this model performs against a confusion matrix.\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 4))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.xticks([0, 1], labels=['Genuine', 'Fraudulent'])\nplt.yticks([0, 1], labels=['Genuine', 'Fraudulent'])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n\n\n\nIn conclusion, we have seen how the performance metric does not give us an idea for the true performance of a classifier. The performance of this model can be better understood by the use of the Chernoff concentration bound. Through visualizations, we illustrated the data distribution, Chernoff probabilities, and model performance. Understanding and applying the Chernoff Inequality can help assess the reliability of machine learning models when dealing with rare events."
  },
  {
    "objectID": "posts/classification/Classification_Final.html",
    "href": "posts/classification/Classification_Final.html",
    "title": "Classifying Bengali MNIST using a Hybrid Quantum Neural Network",
    "section": "",
    "text": "The bengali language, commonly known as bangla is one which is spoken by the people native to the Bengal region of South Asia. It is the sixth most spoken native language and the seventh most spoken language in the world. Machine learning has seen tremendous advancements over the past decade, and one of the most exciting developments in the field is the fusion of quantum computing with traditional neural networks. Quantum computing’s ability to process vast amounts of data and perform complex calculations has opened up new possibilities for solving real-world problems. In this post, we shall see how the juxtaposition of machine learning, quantum computing and the bengali language, i.e., my native language plays out.\nIn this blog, we shall use the bengali MNIST dataset from Kaggle to train a neural network with one convolution layer. But first, let us fetch the data from Kaggle.\n!kaggle datasets download -d truthr/banglamnist\nWe shall now unzip the dataset for our needs.\n!unzip ./banglamnist.zip\nOnce done, we shall import necessary libraries. In our case, we shall be using the torch library to build our neural network. This neural network would have a quantum layer in it which we shall use. This quantum layer is built using the qiskit library.\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport torch\nfrom torch.autograd import Function\nfrom torchvision import datasets, transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchsummary import summary\n\nimport qiskit\nfrom qiskit.visualization import *\n\nfrom PIL import Image\n%matplotlib inline\ntorch.__version__\n\n'1.8.1+cu101'\nIn this notebook, we shall rely on the GPU to speed up the training process for the classical convolution layers. Here we check if the CUDA is available for use by the torch library.\nuse_cuda = torch.cuda.is_available()\n\nprint('CUDA available:', use_cuda)\n\nif use_cuda:\n    device = torch.device('cuda')\n    print('Training on GPU...')\nelse:\n    device = torch.device('cpu')\n    print('Training on CPU...')\n\nCUDA available: True\nTraining on GPU..."
  },
  {
    "objectID": "posts/classification/Classification_Final.html#the-quantum-layer",
    "href": "posts/classification/Classification_Final.html#the-quantum-layer",
    "title": "Classifying Bengali MNIST using a Hybrid Quantum Neural Network",
    "section": "The Quantum Layer",
    "text": "The Quantum Layer\nIn this layer we shall define our quantum layer. Here we shall work with what is known as a quantum circuit for our logic synthesis. We shall input our data into the quantum layer using the what is known as angle encoding, where we use a Hadamard gate, thereby putting it in a state of superposition. Then we embed the features as an angle into the circuit. This will serve as a quantum convolution layer.\n\nclass QuantumCircuit:\n    \"\"\"\n    This class provides a simple interface for interaction\n    with the quantum circuit\n    \"\"\"\n\n    def __init__(self, n_qubits, backend, shots):\n        # --- Circuit definition ---\n        self._circuit = qiskit.QuantumCircuit(n_qubits)\n\n        all_qubits = [i for i in range(n_qubits)]\n        self.theta = qiskit.circuit.Parameter('theta')\n\n        self._circuit.h(all_qubits)\n        self._circuit.barrier()\n        self._circuit.ry(self.theta, all_qubits)\n\n        self._circuit.measure_all()\n        # ---------------------------\n\n        self.backend = backend\n        self.shots = shots\n\n    def run(self, thetas):\n        job = qiskit.execute(self._circuit,\n                             self.backend,\n                             shots = self.shots,\n                             parameter_binds = [{self.theta: theta} for theta in thetas])\n        result = job.result().get_counts(self._circuit)\n\n        counts = np.array(list(result.values()))\n        states = np.array(list(result.keys())).astype(float)\n\n        # Compute probabilities for each state\n        probabilities = counts / self.shots\n        # Get state expectation\n        expectation = np.sum(states * probabilities)\n\n        return np.array([expectation])\n\nWe can measure and visualize the circuit and find the expected value of our embedding.\n\nsimulator = qiskit.Aer.get_backend('qasm_simulator')\n\ncircuit = QuantumCircuit(1, simulator, 100)\nprint('Expected value for rotation pi {}'.format(circuit.run([np.pi])[0]))\ncircuit._circuit.draw('mpl')\n\nExpected value for rotation pi 0.55\n\n\n\n\n\nAll that is left is to interface the quantum layer with the neural network. In this case, we need to define the forward propagation and backward propagation methodology. We define our automatic differentiation for the backpropagation using the gradients to the expected value of the state post-measurement.\n\nclass HybridFunction(Function):\n    \"\"\" Hybrid quantum - classical function definition \"\"\"\n\n    @staticmethod\n    def forward(ctx, inputs, quantum_circuit, shift):\n        \"\"\" Forward pass computation \"\"\"\n        ctx.shift = shift\n        ctx.quantum_circuit = quantum_circuit\n\n        expectation_z = []\n        for input in inputs:\n            expectation_z.append(ctx.quantum_circuit.run(input.tolist()))\n        result = torch.tensor(expectation_z).cuda()\n\n        ctx.save_for_backward(inputs, result)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\" Backward pass computation \"\"\"\n        input, expectation_z = ctx.saved_tensors\n        input_list = np.array(input.tolist())\n\n        shift_right = input_list + np.ones(input_list.shape) * ctx.shift\n        shift_left = input_list - np.ones(input_list.shape) * ctx.shift\n\n        gradients = []\n        for i in range(len(input_list)):\n            expectation_right = ctx.quantum_circuit.run(shift_right[i])\n            expectation_left  = ctx.quantum_circuit.run(shift_left[i])\n\n            gradient = torch.tensor([expectation_right]).cuda() - torch.tensor([expectation_left]).cuda()\n            gradients.append(gradient)\n\n        # gradients = np.array([gradients]).T\n        gradients = torch.tensor([gradients]).cuda()\n        gradients = torch.transpose(gradients, 0, 1)\n\n        # return torch.tensor([gradients]).float() * grad_output.float(), None, None\n        return gradients.float() * grad_output.float(), None, None\n\nclass Hybrid(nn.Module):\n    \"\"\" Hybrid quantum - classical layer definition \"\"\"\n\n    def __init__(self, backend, shots, shift):\n        super(Hybrid, self).__init__()\n        self.quantum_circuit = QuantumCircuit(1, backend, shots)\n        self.shift = shift\n\n    def forward(self, input):\n        return HybridFunction.apply(input, self.quantum_circuit, self.shift)\n\nNow, all that is left is to piece everything together and build the neural network using everything we have so far. We add 2 convolution layer, a dropout layer and at the end, a 10 qubit hybrid quantum convolution layer.\n\nclass Net(nn.Module):\n    def __init__(self, enable_quantum = True):\n        super(Net, self).__init__()\n\n        self.enable_quantum = enable_quantum\n\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.dropout = nn.Dropout2d()\n        self.fc1 = nn.Linear(256, 64)\n        self.fc2 = nn.Linear(64, 10)\n        self.hybrid = [Hybrid(qiskit.Aer.get_backend('qasm_simulator'), 100, np.pi / 2) for i in range(10)]\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = self.dropout(x)\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = torch.chunk(x, 10, dim=1)\n        if self.enable_quantum:\n          x = tuple([hy(x_) for hy, x_ in zip(self.hybrid, x)])\n\n        return torch.cat(x, -1)\n\nA summary of the model is given as follows:\n\nmodel = Net().cuda()\n\nsummary(model, (1, 28, 28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 24, 24]             156\n            Conv2d-2             [-1, 16, 8, 8]           2,416\n         Dropout2d-3             [-1, 16, 4, 4]               0\n            Linear-4                   [-1, 64]          16,448\n            Linear-5                   [-1, 10]             650\n================================================================\nTotal params: 19,670\nTrainable params: 19,670\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.08\nEstimated Total Size (MB): 0.11\n----------------------------------------------------------------\n\n\nNow we use the Adam optimizer and a CrossEntropyLoss to train the model on our dataset. This takes quite a bit of time, even with a GPU.\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# loss_func = nn.NLLLoss()\nloss_func = nn.CrossEntropyLoss().cuda()\n\nepochs = 50\nloss_list = []\n\nmodel.train()\nfor epoch in range(epochs):\n    total_loss = []\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        data = data.cuda()\n        target = target.cuda()\n\n        # Forward pass\n        output = model(data).cuda()\n\n        # print(\"data  \", data.size())\n        # print(\"output\", output.size())\n        # print(\"target\", target.size())\n\n        # Calculating loss\n        loss = loss_func(output, target)\n\n        # Backward pass\n        loss.backward()\n\n        # Optimize the weights\n        optimizer.step()\n\n        total_loss.append(loss.item())\n    loss_list.append(sum(total_loss)/len(total_loss))"
  },
  {
    "objectID": "posts/classification/Classification_Final.html#visualization-and-metrics",
    "href": "posts/classification/Classification_Final.html#visualization-and-metrics",
    "title": "Classifying Bengali MNIST using a Hybrid Quantum Neural Network",
    "section": "Visualization and Metrics",
    "text": "Visualization and Metrics\nNow that we have trained the model, we can see visualize how the model minimizes the losses.\n\ndef show_loss(loss_list):\n  plt.plot(loss_list)\n  plt.title('Hybrid NN Training Convergence')\n  plt.xlabel('Training Iterations')\n  plt.ylabel('Neg Log Likelihood Loss')\n\nText(0, 0.5, 'Neg Log Likelihood Loss')\n\n\n\n\n\nLet us now see the accuracy of the model.\n\ndef evaluate(model):\n  model.eval()\n  with torch.no_grad():\n\n    correct = 0\n    total = 0\n    for batch_idx, (data, target) in enumerate(test_loader):\n\n        data = data.cuda()\n        target = target.cuda()\n\n        output = model(data).cuda()\n\n        pred = output.argmax(dim=1, keepdim=True)\n\n        correct += pred.eq(target.view_as(pred)).sum().item()\n        total += len(target)\n        loss = loss_func(output, target)\n        total_loss.append(loss.item())\n\n    print('Performance on test data:\\n\\tTotal: {:.4f}\\n\\tCorrect: {:.4f}\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n        total,\n        correct,\n        sum(total_loss) / len(total_loss),\n        (correct / total)*100 )\n        )\n\nPerformance on test data:\n    Total: 2048.0000\n    Correct: 1717.0000\n    Loss: 0.5187\n    Accuracy: 83.8%\n\n\nFinally, let us see how the model fares in testing. With an accuracy of around 84%, we dont expect a lot.\n\ndef test(model):\n  n_samples_show = 6\n  count = 0\n  fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n\n  model.eval()\n  print(len(test_loader))\n  with torch.no_grad():\n    for batch_idx, (data, target) in enumerate(test_loader):\n        if count == n_samples_show:\n            break\n\n        data_cuda = data.cuda()\n        target_cuda = target.cuda()\n\n        output_cuda = model(data_cuda).cuda()\n\n        pred = output_cuda.argmax(dim=1, keepdim=True)\n\n        axes[count].imshow(data[count].numpy().squeeze(), cmap='gray')\n\n        axes[count].set_xticks([])\n        axes[count].set_yticks([])\n        axes[count].set_title('Predicted {}'.format(pred[count].item()))\n\n        count += 1\n\n16\n\n\n\n\n\nIn conclusion, we can see that in this case, the quantum layer does not improve the performance by any stretch. This is due to several reasons: There is information loss whenever there we try any kind of embedding technique from classical to quantum data. This could be improved with the introduction of transfer learning but a quantum layer, in this case, does nothing but bring down the accuracy of a model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Turbasu, a second-year PhD student at Virginia Tech.",
    "section": "",
    "text": "I am a Ph.D. student, fortunate enough to be advised by Dr. Jamie Sikora and part of the group QuantumCS@VT. My research lies at the intersection of quantum information theory and online learning techniques. Prior to joining Virginia Tech, I had completed my BTech in computer science and engineering from Maulana Abul Kalam Azad University of Technology, West Bengal, India.\nI am a sucker for jazz and post-rock. I also religiously play CS:GO (now CS2) and an avid meme connoisseur. I will also rarely turn down offers to have a Chipotle bowl."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/anomaly/Anomaly-detection.html",
    "href": "posts/anomaly/Anomaly-detection.html",
    "title": "Anomaly And Outlier Detection In Machine Learning",
    "section": "",
    "text": "In the world of data analysis and machine learning, one of the most critical tasks is the detection of outliers and anomalies in data. In this blog post, we’ll explore how to use machine learning techniques to identify outliers and anomalies in data and provide code examples using Python. However, before we do any of that, it is important to define the ideas of anomalies and outliers statistically. Turns out, this is pretty ambiguous, so we will try to clear them up here:\nIn this article, we shall employ two elementary methods for outlier detection: The K-Nearest Neighbors (KNN) algorithm and the Isolation Forest algorithm."
  },
  {
    "objectID": "posts/anomaly/Anomaly-detection.html#the-k-nearest-neighbor-algorithm-for-outlier-detection",
    "href": "posts/anomaly/Anomaly-detection.html#the-k-nearest-neighbor-algorithm-for-outlier-detection",
    "title": "Anomaly And Outlier Detection In Machine Learning",
    "section": "The K-Nearest Neighbor Algorithm For Outlier Detection",
    "text": "The K-Nearest Neighbor Algorithm For Outlier Detection\nIn the realm of supervised learning, the K-Nearest Neighbors (KNN) algorithm stands as a fundamental yet powerful method for classification and regression tasks. This algorithm, rooted in the concept of similarity, is intuitive, versatile, and widely employed across various domains. At its core, KNN is a non-parametric, lazy learning algorithm used for both classification and regression. It operates on the principle of proximity, where the class or value of an unseen data point is determined by the classes or values of its nearest neighbors in the training dataset.\nLet us take a closer look at the inner working of the KNN algorithm. There are 3 main components of the KNN algorithm that we need to learn before we progress:\n\nFor an unseen data point, the KNN computes its distance to all points in the training data. This distance maybe calculated using norms like euclidean norm, minkowski norm, manhattan norm, etc..\nNow the algorithm shall identify the K closest data points based on the calculated distances.\nIt will then assign a class that is most common for the K neighbors.\n\nLet us put theory to practice and use the scikit-learn to identify anomalies in the iris dataset.\n\n# Imports\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nLet us load the Iris dataset now.\n\niris = load_iris()\ndata = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                    columns= iris['feature_names'] + ['target'])\n\nWe shall now only use numeric data to make it easier on ourselves.\n\n# Consider only numeric features for simplicity\nnumeric_data = data.drop('target', axis=1)\n\nLets now invoke the KNN algorithm and set our k to be equal to 5 and fit the algorithm on the numeric dataset that we have now. We will set our k to be equal to 5 in our case.\n\nk = 5  # Number of neighbors\nknn = NearestNeighbors(n_neighbors=k)\nknn.fit(numeric_data)\n\nNearestNeighbors()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.NearestNeighborsNearestNeighbors()\n\n\nNow, lets calculate the distance to the k neighbors\n\ndistances, indices = knn.kneighbors()\n\nLet us see what the distances look like now:\n\ndistances\n\narray([[0.1       , 0.14142136, 0.14142136, 0.14142136, 0.14142136],\n       [0.14142136, 0.14142136, 0.14142136, 0.17320508, 0.2236068 ],\n       [0.14142136, 0.24494897, 0.26457513, 0.26457513, 0.26457513],\n       [0.14142136, 0.17320508, 0.2236068 , 0.24494897, 0.26457513],\n       [0.14142136, 0.14142136, 0.17320508, 0.17320508, 0.2236068 ],\n       [0.33166248, 0.34641016, 0.36055513, 0.37416574, 0.38729833],\n       [0.2236068 , 0.26457513, 0.3       , 0.31622777, 0.31622777],\n       [0.1       , 0.14142136, 0.17320508, 0.2       , 0.2236068 ],\n       [0.14142136, 0.3       , 0.31622777, 0.34641016, 0.36055513],\n       [0.1       , 0.17320508, 0.17320508, 0.17320508, 0.2       ],\n       [0.1       , 0.28284271, 0.3       , 0.33166248, 0.33166248],\n       [0.2236068 , 0.2236068 , 0.28284271, 0.3       , 0.3       ],\n       [0.14142136, 0.17320508, 0.2       , 0.2       , 0.24494897],\n       [0.24494897, 0.31622777, 0.34641016, 0.47958315, 0.5       ],\n       [0.41231056, 0.46904158, 0.54772256, 0.55677644, 0.58309519],\n       [0.36055513, 0.54772256, 0.6164414 , 0.6164414 , 0.64031242],\n       [0.34641016, 0.36055513, 0.38729833, 0.38729833, 0.4       ],\n       [0.1       , 0.14142136, 0.17320508, 0.17320508, 0.17320508],\n       [0.33166248, 0.38729833, 0.46904158, 0.50990195, 0.51961524],\n       [0.14142136, 0.14142136, 0.24494897, 0.26457513, 0.31622777],\n       [0.28284271, 0.3       , 0.36055513, 0.36055513, 0.36055513],\n       [0.14142136, 0.24494897, 0.24494897, 0.26457513, 0.28284271],\n       [0.45825757, 0.50990195, 0.50990195, 0.51961524, 0.53851648],\n       [0.2       , 0.26457513, 0.37416574, 0.38729833, 0.38729833],\n       [0.3       , 0.37416574, 0.41231056, 0.42426407, 0.4472136 ],\n       [0.17320508, 0.2       , 0.2236068 , 0.2236068 , 0.3       ],\n       [0.2       , 0.2236068 , 0.2236068 , 0.24494897, 0.26457513],\n       [0.14142136, 0.14142136, 0.14142136, 0.17320508, 0.2236068 ],\n       [0.14142136, 0.14142136, 0.14142136, 0.17320508, 0.2236068 ],\n       [0.14142136, 0.17320508, 0.2236068 , 0.2236068 , 0.24494897],\n       [0.14142136, 0.14142136, 0.17320508, 0.2236068 , 0.2236068 ],\n       [0.28284271, 0.3       , 0.3       , 0.31622777, 0.34641016],\n       [0.34641016, 0.34641016, 0.37416574, 0.42426407, 0.45825757],\n       [0.34641016, 0.36055513, 0.38729833, 0.41231056, 0.47958315],\n       [0.1       , 0.14142136, 0.14142136, 0.17320508, 0.2       ],\n       [0.2236068 , 0.3       , 0.31622777, 0.33166248, 0.33166248],\n       [0.3       , 0.31622777, 0.33166248, 0.34641016, 0.36055513],\n       [0.14142136, 0.24494897, 0.26457513, 0.26457513, 0.3       ],\n       [0.14142136, 0.2       , 0.24494897, 0.3       , 0.3       ],\n       [0.1       , 0.14142136, 0.14142136, 0.14142136, 0.17320508],\n       [0.14142136, 0.17320508, 0.17320508, 0.24494897, 0.24494897],\n       [0.6244998 , 0.71414284, 0.76811457, 0.78102497, 0.79372539],\n       [0.2       , 0.2236068 , 0.3       , 0.3       , 0.31622777],\n       [0.2236068 , 0.26457513, 0.31622777, 0.37416574, 0.42426407],\n       [0.36055513, 0.37416574, 0.41231056, 0.41231056, 0.47958315],\n       [0.14142136, 0.2       , 0.2       , 0.24494897, 0.26457513],\n       [0.14142136, 0.24494897, 0.24494897, 0.3       , 0.33166248],\n       [0.14142136, 0.14142136, 0.2236068 , 0.2236068 , 0.2236068 ],\n       [0.1       , 0.2236068 , 0.24494897, 0.24494897, 0.28284271],\n       [0.14142136, 0.17320508, 0.2236068 , 0.2236068 , 0.2236068 ],\n       [0.26457513, 0.33166248, 0.43588989, 0.45825757, 0.51961524],\n       [0.26457513, 0.31622777, 0.34641016, 0.37416574, 0.38729833],\n       [0.26457513, 0.28284271, 0.31622777, 0.34641016, 0.50990195],\n       [0.2       , 0.3       , 0.31622777, 0.43588989, 0.43588989],\n       [0.24494897, 0.31622777, 0.37416574, 0.37416574, 0.38729833],\n       [0.3       , 0.31622777, 0.31622777, 0.33166248, 0.37416574],\n       [0.26457513, 0.37416574, 0.42426407, 0.45825757, 0.45825757],\n       [0.14142136, 0.38729833, 0.45825757, 0.72111026, 0.78740079],\n       [0.24494897, 0.24494897, 0.31622777, 0.31622777, 0.31622777],\n       [0.38729833, 0.50990195, 0.51961524, 0.52915026, 0.53851648],\n       [0.36055513, 0.45825757, 0.67082039, 0.71414284, 0.72111026],\n       [0.3       , 0.33166248, 0.36055513, 0.36055513, 0.37416574],\n       [0.48989795, 0.51961524, 0.54772256, 0.58309519, 0.58309519],\n       [0.14142136, 0.2236068 , 0.24494897, 0.42426407, 0.43588989],\n       [0.42426407, 0.4472136 , 0.50990195, 0.51961524, 0.53851648],\n       [0.14142136, 0.31622777, 0.31622777, 0.34641016, 0.38729833],\n       [0.2       , 0.3       , 0.38729833, 0.41231056, 0.42426407],\n       [0.24494897, 0.28284271, 0.33166248, 0.36055513, 0.37416574],\n       [0.26457513, 0.50990195, 0.53851648, 0.678233  , 0.70710678],\n       [0.17320508, 0.24494897, 0.26457513, 0.26457513, 0.3       ],\n       [0.2236068 , 0.3       , 0.36055513, 0.42426407, 0.46904158],\n       [0.33166248, 0.34641016, 0.37416574, 0.4       , 0.41231056],\n       [0.36055513, 0.36055513, 0.41231056, 0.42426407, 0.43588989],\n       [0.2236068 , 0.3       , 0.38729833, 0.43588989, 0.45825757],\n       [0.2       , 0.26457513, 0.36055513, 0.38729833, 0.38729833],\n       [0.14142136, 0.24494897, 0.26457513, 0.31622777, 0.31622777],\n       [0.31622777, 0.34641016, 0.34641016, 0.37416574, 0.42426407],\n       [0.31622777, 0.37416574, 0.41231056, 0.42426407, 0.42426407],\n       [0.2       , 0.24494897, 0.33166248, 0.34641016, 0.37416574],\n       [0.34641016, 0.42426407, 0.43588989, 0.4472136 , 0.46904158],\n       [0.14142136, 0.17320508, 0.3       , 0.3       , 0.42426407],\n       [0.14142136, 0.26457513, 0.34641016, 0.43588989, 0.43588989],\n       [0.14142136, 0.26457513, 0.28284271, 0.3       , 0.34641016],\n       [0.33166248, 0.36055513, 0.36055513, 0.37416574, 0.41231056],\n       [0.2       , 0.41231056, 0.47958315, 0.48989795, 0.50990195],\n       [0.37416574, 0.42426407, 0.45825757, 0.46904158, 0.50990195],\n       [0.28284271, 0.31622777, 0.31622777, 0.33166248, 0.34641016],\n       [0.26457513, 0.57445626, 0.59160798, 0.60827625, 0.6164414 ],\n       [0.17320508, 0.17320508, 0.2236068 , 0.31622777, 0.37416574],\n       [0.2       , 0.24494897, 0.3       , 0.3       , 0.33166248],\n       [0.26457513, 0.31622777, 0.42426407, 0.42426407, 0.42426407],\n       [0.14142136, 0.2       , 0.3       , 0.34641016, 0.38729833],\n       [0.14142136, 0.24494897, 0.26457513, 0.26457513, 0.31622777],\n       [0.14142136, 0.36055513, 0.38729833, 0.64807407, 0.72111026],\n       [0.17320508, 0.2236068 , 0.26457513, 0.3       , 0.31622777],\n       [0.14142136, 0.17320508, 0.24494897, 0.33166248, 0.36055513],\n       [0.14142136, 0.14142136, 0.17320508, 0.2236068 , 0.3       ],\n       [0.2       , 0.33166248, 0.34641016, 0.34641016, 0.38729833],\n       [0.38729833, 0.38729833, 0.72111026, 0.79372539, 0.81853528],\n       [0.14142136, 0.17320508, 0.2236068 , 0.24494897, 0.26457513],\n       [0.42426407, 0.5       , 0.50990195, 0.55677644, 0.60827625],\n       [0.        , 0.26457513, 0.31622777, 0.33166248, 0.36055513],\n       [0.38729833, 0.4       , 0.41231056, 0.45825757, 0.5       ],\n       [0.24494897, 0.24494897, 0.33166248, 0.38729833, 0.42426407],\n       [0.3       , 0.31622777, 0.36055513, 0.38729833, 0.38729833],\n       [0.26457513, 0.52915026, 0.54772256, 0.54772256, 0.60827625],\n       [0.73484692, 0.76157731, 0.79372539, 0.87749644, 0.88317609],\n       [0.26457513, 0.43588989, 0.52915026, 0.54772256, 0.55677644],\n       [0.55677644, 0.6       , 0.6164414 , 0.6164414 , 0.6244998 ],\n       [0.63245553, 0.67082039, 0.70710678, 0.75498344, 0.80622577],\n       [0.2236068 , 0.37416574, 0.42426407, 0.42426407, 0.46904158],\n       [0.34641016, 0.37416574, 0.37416574, 0.38729833, 0.38729833],\n       [0.17320508, 0.34641016, 0.36055513, 0.37416574, 0.37416574],\n       [0.26457513, 0.26457513, 0.33166248, 0.51961524, 0.54772256],\n       [0.48989795, 0.50990195, 0.50990195, 0.51961524, 0.64031242],\n       [0.3       , 0.37416574, 0.37416574, 0.38729833, 0.38729833],\n       [0.14142136, 0.24494897, 0.36055513, 0.38729833, 0.38729833],\n       [0.41231056, 0.81853528, 0.86023253, 1.00498756, 1.0198039 ],\n       [0.41231056, 0.54772256, 0.89442719, 0.92736185, 0.96436508],\n       [0.43588989, 0.51961524, 0.53851648, 0.58309519, 0.65574385],\n       [0.2236068 , 0.26457513, 0.3       , 0.3       , 0.36055513],\n       [0.31622777, 0.31622777, 0.33166248, 0.45825757, 0.48989795],\n       [0.26457513, 0.41231056, 0.60827625, 0.678233  , 0.7       ],\n       [0.17320508, 0.24494897, 0.36055513, 0.36055513, 0.37416574],\n       [0.3       , 0.31622777, 0.37416574, 0.37416574, 0.38729833],\n       [0.34641016, 0.38729833, 0.43588989, 0.46904158, 0.64807407],\n       [0.17320508, 0.24494897, 0.28284271, 0.38729833, 0.42426407],\n       [0.14142136, 0.24494897, 0.28284271, 0.3       , 0.36055513],\n       [0.1       , 0.31622777, 0.33166248, 0.37416574, 0.38729833],\n       [0.34641016, 0.50990195, 0.51961524, 0.55677644, 0.70710678],\n       [0.26457513, 0.45825757, 0.46904158, 0.50990195, 0.53851648],\n       [0.41231056, 0.88317609, 0.92736185, 0.93273791, 1.02469508],\n       [0.1       , 0.3       , 0.42426407, 0.43588989, 0.46904158],\n       [0.33166248, 0.36055513, 0.37416574, 0.43588989, 0.45825757],\n       [0.53851648, 0.55677644, 0.58309519, 0.66332496, 0.7       ],\n       [0.53851648, 0.54772256, 0.66332496, 0.678233  , 0.7       ],\n       [0.24494897, 0.38729833, 0.42426407, 0.43588989, 0.5       ],\n       [0.14142136, 0.24494897, 0.38729833, 0.43588989, 0.45825757],\n       [0.14142136, 0.2236068 , 0.28284271, 0.31622777, 0.43588989],\n       [0.17320508, 0.36055513, 0.36055513, 0.37416574, 0.41231056],\n       [0.24494897, 0.26457513, 0.34641016, 0.34641016, 0.36055513],\n       [0.24494897, 0.36055513, 0.46904158, 0.50990195, 0.51961524],\n       [0.        , 0.26457513, 0.31622777, 0.33166248, 0.36055513],\n       [0.2236068 , 0.31622777, 0.31622777, 0.34641016, 0.38729833],\n       [0.24494897, 0.3       , 0.31622777, 0.4       , 0.43588989],\n       [0.24494897, 0.36055513, 0.36055513, 0.37416574, 0.37416574],\n       [0.24494897, 0.37416574, 0.38729833, 0.41231056, 0.47958315],\n       [0.2236068 , 0.34641016, 0.36055513, 0.36055513, 0.38729833],\n       [0.24494897, 0.3       , 0.55677644, 0.6164414 , 0.6244998 ],\n       [0.28284271, 0.31622777, 0.33166248, 0.33166248, 0.36055513]])\n\n\nNow we calculate outlier scores based on the distance metric.\n\noutlier_scores = distances[:, -1]\n\nNow, we shall set our threshold for the outliers. This can be adjusted accordingly, but for now, we shall keep our threshold at the 95th percentile.\n\nthreshold = np.percentile(outlier_scores, 95)\n\nNow let’s see what our outliers are.\n\noutliers = np.where(outlier_scores &gt; threshold)[0]\n\nNow let’s visualize the outliers that we have found so far:\n\n\nplt.figure(figsize=(8, 6))\n\nplt.scatter(numeric_data.iloc[:, 0], numeric_data.iloc[:, 1], label='Data Points', alpha=0.7)\nplt.scatter(numeric_data.iloc[outliers, 0], numeric_data.iloc[outliers, 1],\n            c='r', s=100, label='Outliers')\n\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('Outlier Detection with KNN')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/anomaly/Anomaly-detection.html#the-isolation-forest-algorithm-for-anomalies",
    "href": "posts/anomaly/Anomaly-detection.html#the-isolation-forest-algorithm-for-anomalies",
    "title": "Anomaly And Outlier Detection In Machine Learning",
    "section": "The Isolation Forest Algorithm For Anomalies",
    "text": "The Isolation Forest Algorithm For Anomalies\nWe now use the Isolation Forest algorithm for classifying anomalies. Developed by Liu, Ting, and Zhou in 2008, the IsolationForest algorithm stands out for its ability to efficiently isolate anomalies within a dataset. Unlike traditional distance or density-based methods, IsolationForest utilizes a tree-based structure to identify anomalies based on their isolation from the majority of data points.\nThe key concepts used in the Isolation Forest algorithm are:\n\nIsolationForest randomly selects features and splits data points, recursively creating isolation trees. This is called random partitioning.\nAnomalies are expected to have shorter path lengths in the trees, resulting in fewer partitions to be isolated.\n\nThe methodology for the Isolation Forest is as follows: First we select features and split the data points until they are isolated or a specific tree length is reached. Now we calculate the average path length to isolate each data point across multiple trees. Then, as mentioned before, shorter path lengths are considered to be anomalies.\nNow, we shall put this theory into practice. Using the same dataset, we shall invoke an IsolationForest object to classify the numeric dataset.\n\nfrom sklearn.ensemble import IsolationForest\n\n# Create IsolationForest model\nclf = IsolationForest()\nclf.fit(numeric_data)\n\nIsolationForest()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest()\n\n\nLets see what are the outliers in the dataset.\n\noutliers = clf.predict(numeric_data)\n\nNow lets visualize the results:\n\nplt.figure(figsize=(8, 6))\nplt.scatter(numeric_data.iloc[:, 0], numeric_data.iloc[:, 1], c=outliers, cmap='viridis')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('Outlier Detection in Iris Dataset using IsolationForest')\nplt.colorbar(label='Outlier Score')\nplt.show()\n\n\n\n\nHere darker shades represents higher outlier scores, which means they are definitely outliers.\nTherefore, we have seen how we can perform outlier detection in python with these elementary algorithms and we have visualized the effectiveness of the algorithms."
  },
  {
    "objectID": "posts/clustering/Clustering.html",
    "href": "posts/clustering/Clustering.html",
    "title": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "section": "",
    "text": "The DBSCAN algorithm, developed by Martin Ester, Hans-Peter Kriegel, Jorg Sander and Xiaowei Xu in 1996, is a powerful clustering algorithm that identifies clusters of varying shapes and sizes within data. DBSCAN can discover clusters based on the density of data points, making it particularly useful in scenarios where the number of clusters is unknown or when dealing with noisy data. The algorithm groups together points that are tightly packed and treats those points that are in low density regions as outliers. It also does not require the number of clusters to be told beforehand, unlike K-Means, where we have to specify the number of centroids. DBSCAN is one of the most commonly used and cited clustering algorithms.\nAt the heart of DBSCAN lies two key parameters: \\(\\varepsilon\\) which defines the radius of the search and minPoints, the minimum number of points required for the algorithm to deem it a cluster.\nFurther, we have the following core concepts that will come handy when trying to understand what the algorithm does:\nThe DBSCAN algorithm can be outlined in the pseudocode below:\nThe key mathematical operations in the pseudocode are the distance calculation (used to determine ϵ-reachability), the count of reachable points, and the clustering process.\nLet’s put theory to practice and code it out!"
  },
  {
    "objectID": "posts/clustering/Clustering.html#implementing-the-dbscan-from-scratch",
    "href": "posts/clustering/Clustering.html#implementing-the-dbscan-from-scratch",
    "title": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "section": "Implementing the DBSCAN from Scratch",
    "text": "Implementing the DBSCAN from Scratch\nIn this section, we would actually put theory to practice and code the DBSCAN algorithm from scratch and see how well the algorithm performs!\n\n# Necessary imports\n\nimport numpy as np\n\nWe’ll start off by coding up the helper function for calculating the euclidean distance.\n\ndef calculate_euclidean_distance(point1, point2):\n    return np.linalg.norm(point1 - point2)\n\nNext, we shall code up the part where we find the core points for the core functionality of the clustering algorithm.\n\ndef find_core_points(data, epsilon, min_points):\n    core_points = set()\n    for i, point in enumerate(data):\n        if len([p for p in data if calculate_euclidean_distance(point, p) &lt;= epsilon]) &gt;= min_points:\n            core_points.add(i)\n    return core_points\n\nNow we shall move on to implementing the real algorithm for clustering.\n\ndef dbscan(data, epsilon, min_points):\n    labels = [-1] * len(data)  # Initialize all points as noise (-1)\n\n    core_points = find_core_points(data, epsilon, min_points)\n    cluster_id = 0\n\n    for i, point in enumerate(data):\n        if labels[i] != -1 or i not in core_points:\n            continue\n\n        cluster_id += 1\n        stack = [i]\n\n        while stack:\n            current_point = stack.pop()\n            labels[current_point] = cluster_id\n            neighbors = [p for p in range(len(data)) if calculate_euclidean_distance(data[current_point], data[p]) &lt;= epsilon]\n\n            for neighbor in neighbors:\n                if labels[neighbor] == -1:\n                    labels[neighbor] = cluster_id\n                    if neighbor in core_points:\n                        stack.append(neighbor)\n\n    return labels"
  },
  {
    "objectID": "posts/clustering/Clustering.html#comparing-the-performance-of-our-algorithm-to-the-k-means",
    "href": "posts/clustering/Clustering.html#comparing-the-performance-of-our-algorithm-to-the-k-means",
    "title": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "section": "Comparing The Performance Of Our Algorithm To The K-Means",
    "text": "Comparing The Performance Of Our Algorithm To The K-Means\nNow that it is done, we shall pit the DBSCAN that we coded up and compare it to the performance of the K-Means algorithm from the scikit-learn package. In order to do this, we shall load the Iris dataset from the scikit-learn package. Let’s do that below:\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nNow that we have our dataset loaded and ready, we shall scale the datapoints to fit the standard normal distribution using the StandardScalar object.\n\n# Scale the features for better clustering performance\nX = StandardScaler().fit_transform(X)\n\nNow we shall perform our PCA for 2D visualization\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nLet’s apply the K-Means to the dataset\n\n# K-Means Clustering\nkmeans = KMeans(n_clusters=3)\nkmeans_labels = kmeans.fit_predict(X)\n\nC:\\Users\\turba\\anaconda3\\envs\\quantum-experiments\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nNow we shall apply our code for DBSCAN clustering of the data\n\nepsilon = 0.5\nmin_samples = 5\ndbscan_labels = np.asarray(dbscan(X, epsilon=epsilon, min_points=min_samples))\n\nNow let’s visualize the results of each!\n\n# Create subplots for K-Means and DBSCAN visualizations\nplt.figure(figsize=(12, 5))\n\n# K-Means Visualization\nplt.subplot(1, 2, 1)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis')\nplt.title('K-Means Clustering')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\n\n# DBSCAN Visualization\nplt.subplot(1, 2, 2)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='viridis')\nplt.title('DBSCAN Clustering')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/regression/Regression.html",
    "href": "posts/regression/Regression.html",
    "title": "Linear and Non-Linear Regression: A Performance Comparison",
    "section": "",
    "text": "Regression analysis is a basic yet powerful tool in the field of statistics and machine learning that allows us to probe in and model the the relationship between one or more independent variables and a dependent variable. Here, we shall take a look at linear and non-linear regression and the differences in performance.\nIn order to elucidate, we shall use the USA Housing Prices dataset from Kaggle. However, before we get our hands dirty into the math and code, let us load our dataset and perform some exploratory data analysis."
  },
  {
    "objectID": "posts/regression/Regression.html#data-loading-and-eda",
    "href": "posts/regression/Regression.html#data-loading-and-eda",
    "title": "Linear and Non-Linear Regression: A Performance Comparison",
    "section": "Data Loading and EDA",
    "text": "Data Loading and EDA\nIn the python cells below, we shall call the necessary libraries to perform our analysis. Further, we shall first call our dataset and explore its characteristics to inspect it.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\n%matplotlib inline\n\n# Fancy plots\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\n\nUSAhousing = pd.read_csv('USA_Housing.csv')\n\n\nUSAhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 7 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   Avg. Area Income              5000 non-null   float64\n 1   Avg. Area House Age           5000 non-null   float64\n 2   Avg. Area Number of Rooms     5000 non-null   float64\n 3   Avg. Area Number of Bedrooms  5000 non-null   float64\n 4   Area Population               5000 non-null   float64\n 5   Price                         5000 non-null   float64\n 6   Address                       5000 non-null   object \ndtypes: float64(6), object(1)\nmemory usage: 273.6+ KB\n\n\n\nUSAhousing.columns\n\nIndex(['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n       'Avg. Area Number of Bedrooms', 'Area Population', 'Price', 'Address'],\n      dtype='object')\n\n\n\nUSAhousing.describe()\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\n\n\n\n\ncount\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5.000000e+03\n\n\nmean\n68583.108984\n5.977222\n6.987792\n3.981330\n36163.516039\n1.232073e+06\n\n\nstd\n10657.991214\n0.991456\n1.005833\n1.234137\n9925.650114\n3.531176e+05\n\n\nmin\n17796.631190\n2.644304\n3.236194\n2.000000\n172.610686\n1.593866e+04\n\n\n25%\n61480.562388\n5.322283\n6.299250\n3.140000\n29403.928702\n9.975771e+05\n\n\n50%\n68804.286404\n5.970429\n7.002902\n4.050000\n36199.406689\n1.232669e+06\n\n\n75%\n75783.338666\n6.650808\n7.665871\n4.490000\n42861.290769\n1.471210e+06\n\n\nmax\n107701.748378\n9.519088\n10.759588\n6.500000\n69621.713378\n2.469066e+06\n\n\n\n\n\n\n\nLet us see now plot the data and see if there exists any pairwise relationship in our data.\n\nUSAhousing.hvplot.hist(\"Price\")\n\n\n\n\n\n\n  \n\n\n\n\n\nUSAhousing.hvplot.scatter(x='Avg. Area House Age', y='Price')\n\n\n\n\n\n\n  \n\n\n\n\n\nUSAhousing.hvplot.scatter(x='Avg. Area Income', y='Price')\n\n\n\n\n\n\n  \n\n\n\n\n\n## Drop address column\nUSAHousingAddressDropped = USAhousing.drop('Address', axis=1)\n\n# Visualise a pairwise correlation of every column (if exists)\nsns.heatmap(USAHousingAddressDropped.corr())\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/regression/Regression.html#nailing-down-the-basics",
    "href": "posts/regression/Regression.html#nailing-down-the-basics",
    "title": "Linear and Non-Linear Regression: A Performance Comparison",
    "section": "Nailing down the basics",
    "text": "Nailing down the basics\nLinear regression is a fundamental statistical technique that plays a pivotal role in data analysis, predictive modeling, and machine learning. It serves as a foundation for understanding the relationship between variables and making predictions based on that relationship. In this article, we will explore the mathematical background of linear regression, helping you grasp the core concepts behind this powerful tool.\nLinear regression aims to model the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data. This equation takes the form: \\[\\begin{equation}\n    \\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\ldots + b_n x_n + \\varepsilon\n\\end{equation}\\] where \\(x_i\\) is the \\(i^{th}\\) independent variable, \\(b_i\\) is the coefficient of the \\(i^{th}\\) independent variable, and \\(\\varepsilon\\) represents the error term i.e. the difference between the predicted and the actual values.\nThe goal of linear regression is to estimate the values of \\(b_1, b_2 \\ldots b_n\\) and \\(a\\) that best fit the data at hand. We can determine these values using the mean squared error estimation method. This can be done by to fit the data as follows:\n\\[\\begin{equation}\n    \\min \\limits_{\\mathbf{b}} \\frac 1n S(\\mathbf{b}) = \\frac 1n \\sum \\limits_{i=1}^n (y_i - \\hat{y_i})^2 = \\frac 1n \\sum \\limits_{i=1}^n e_i^2,\n\\end{equation}\\]\nwhere for each \\(1 \\leq i \\leq n\\),\n\\[\\begin{equation}\n    \\hat{y}_i = \\hat{b_0} + \\hat{b_1} x_{i1} + \\ldots + \\hat{b_n}x_{in}\n\\end{equation}\\]\nNow, let \\(\\mathbf{e} = (e_i) \\in \\mathbb{R}\\) and \\(\\hat{\\mathbf{y}} = (\\hat{y_i}) = \\mathbf{X \\hat{b}} \\in \\mathbb{R}^n\\). Then \\(\\mathbb{e = y - \\hat{y}}\\). This boils the above problem to the following:\n\\[\\begin{equation}\n    \\min \\limits_{\\mathbf{\\hat{b}}} \\frac 1n S (\\mathbf{\\hat{b}}) = \\frac 1n ||\\mathbf{e}||^2 = \\frac 1n || \\mathbf{y - X \\hat{b}} ||^2.\n\\end{equation}\\]\nBesides, this methodology, we shall apply two more methods to evaluate our regression problem:\n\nAbsolute Error (MAE) is the mean of the absolute value of the errors: \\[\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|.\\]\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors: \\[\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}.\\]\n\nTogether, these three functions shall serve as our loss functions. However, as we have seen, the goal shall remain the same: We need to minimize the loss function.\n\nX = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n               'Avg. Area Number of Bedrooms', 'Area Population']]\ny = USAhousing['Price']\n\nNow, let’s split the data into a training set and a testing set. We will train our model on the training set and then use the test set to evaluate the model.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square\n\nHere we shall add a little bit of preprocessing by scaling and shifting the values in our dataset to constitute that of a standard normal distribution with mean 0 and variance 1. After this step we shall add them to our pipeline which shall help us plan out this entire study.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('std_scalar', StandardScaler())\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)\n\nHere, we shall invoke the LinearRegression() method that calls upon the linear regressor and have that trained on our dataset \\(X\\) with our target values for \\(y\\).\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nNow, let’s evaluate the model by checking out it’s coefficients and how we can interpret them. We begin by printing our \\(y\\)-intercept, i.e. our value for \\(b_0\\) as follows:\n\n# print the intercept\nprint(lin_reg.intercept_)\n\n1228219.1492415662\n\n\nThese are the values for \\(b_1, \\ldots, b_n\\) which are our regression coefficients for average area’s income, average area’s house age, average areas number of rooms, average number of bedrooms and population for the area.\n\ncoeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nAvg. Area Income\n232679.724643\n\n\nAvg. Area House Age\n163841.046593\n\n\nAvg. Area Number of Rooms\n121110.555478\n\n\nAvg. Area Number of Bedrooms\n2892.815119\n\n\nArea Population\n151252.342377\n\n\n\n\n\n\n\nInterpreting the coefficients: - Holding all other features fixed, a 1 unit increase in Avg. Area Income is associated with an increase of $ 21.52 - Holding all other features fixed, a 1 unit increase in Avg. Area House Age is associated with an increase of $ 164883.28. - Holding all other features fixed, a 1 unit increase in Avg. Area Number of Rooms is associated with an increase of $ 122368.67. - Holding all other features fixed, a 1 unit increase in Avg. Area Number of Bedrooms is associated with an increase of $ 2233.80. - Holding all other features fixed, a 1 unit increase in Area Population is associated with an increase of $ 15.15.\nNow that we have trained our model to the dataset and have obtained the values of \\(b_0 \\ldots b_n\\), let’s grab predictions off our test set and see how well it did!\n\npred = lin_reg.predict(X_test)\n\n\npd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')\n\n\n\n\n\n\n  \n\n\n\n\nResidual Histogram\nLet’s see how our errors pan out.\n\npd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde()\n\n\n\n\n\n\n  \n\n\n\n\nLet’s see the optimal values that we had for the Root Mean Square Error, Mean Squared Error and Mean Absolute Error when training as well as testing.\n\ntest_pred = lin_reg.predict(X_test)\ntrain_pred = lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n\nTest set evaluation:\n_____________________________________\nMAE: 81135.56609336878\nMSE: 10068422551.40088\nRMSE: 100341.52954485436\nR2 Square 0.9146818498754016\n__________________________________\nTrain set evaluation:\n_____________________________________\nMAE: 81480.4997317489\nMSE: 10287043161.197224\nRMSE: 101425.06180031257\nR2 Square 0.9192986579075526\n__________________________________"
  },
  {
    "objectID": "posts/regression/Regression.html#ridge-and-lasso-regression",
    "href": "posts/regression/Regression.html#ridge-and-lasso-regression",
    "title": "Linear and Non-Linear Regression: A Performance Comparison",
    "section": "Ridge and Lasso Regression",
    "text": "Ridge and Lasso Regression\nBefore diving into Ridge and Lasso, let’s understand why we need these regularization techniques in the first place. In a standard linear regression model, we aim to find the coefficients for each predictor variable that minimize the sum of squared residuals. The problem arises when we have many predictor variables or when some of them are highly correlated. In such cases, the model tends to overfit, resulting in poor generalization to new, unseen data. Overfitting is a common problem in machine learning. It occurs when a model fits the training data so closely that it captures noise rather than the underlying relationships in the data. As a result, the model performs poorly on unseen data, a situation known as poor generalization.\nRegularization methods introduce a penalty term to the traditional least squares objective function, discouraging the model from assigning high values to the coefficients of predictor variables. Ridge and Lasso are two popular regularization techniques that help us tackle these issues. Ridge and Lasso regression are both linear regression techniques that combat overfitting by adding a regularization term to the linear regression cost function. This regularization term is based on Tikhonov regularization, named after the Russian mathematician Andrey Tikhonov. Tikhonov regularization, also known as L2 regularization, minimizes the sum of the squared coefficients, adding a penalty term to the cost function.\nThe cost function for ridge regression is expressed as follows: \\[\\begin{equation}\n    J(b) = MSE + \\alpha \\sum \\limits_{i=1}^n b_i^2,\n\\end{equation}\\]\nwhere \\(J(\\beta)\\) is the cost function, \\(\\alpha\\) is the regularization parameter that controls the strength of the regularization, \\(b_i\\) are the model coefficients and MSE is the Mean Squared Error, measuring the model’s error on the training data.\nOn the other hand, Lasso regression (Least Absolute Shrinkage and Selection Operator) uses L1 regularization, which minimizes the absolute values of the coefficients and can lead to sparsity (i.e., some coefficients being exactly zero). The Lasso cost function is expressed as:\n\\[\\begin{equation}\n    J(b) = MSE + \\alpha \\sum \\limits_{i=1}^n |b_i|.\n\\end{equation}\\]\nLets apply the theory above into practice by calling a Ridge object from scikit-learn for Ridge regression and fitting it to our dataset. We then evaluate the model using the same metrics of MAE, MSE, etc. and append it to a resultant dataframe for future reference.\n\nfrom sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n#\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n#\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n#\nresults_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]],\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n\nresults_df = pd.concat([results_df, results_df_2])\nresults_df\n\nTest set evaluation:\n_____________________________________\nMAE: 81428.64835535336\nMSE: 10153269900.89261\nRMSE: 100763.43533689495\nR2 Square 0.9139628674464607\n__________________________________\n====================================\nTrain set evaluation:\n_____________________________________\nMAE: 81972.39058585512\nMSE: 10382929615.14346\nRMSE: 101896.66145239233\nR2 Square 0.9185464334441484\n__________________________________\n\n\n\n\n\n\n\n\n\nModel\nMAE\nMSE\nRMSE\nR2 Square\nCross Validation\n\n\n\n\n0\nLinear Regression\n81135.566093\n1.006842e+10\n100341.529545\n0.914682\n0.917379\n\n\n0\nRidge Regression\n81428.648355\n1.015327e+10\n100763.435337\n0.913963\n0.917379\n\n\n\n\n\n\n\nWe do the same for the Lasso regression model by calling in a Lasso object from scikit-learn for our needs.\n\nfrom sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, \n              precompute=True, \n#               warm_start=True, \n              positive=True, \n              selection='random',\n              random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = pd.concat([results_df, results_df_2])\n\nresults_df\n\nTest set evaluation:\n_____________________________________\nMAE: 81135.6985172622\nMSE: 10068453390.364521\nRMSE: 100341.68321472648\nR2 Square 0.914681588551116\n__________________________________\n====================================\nTrain set evaluation:\n_____________________________________\nMAE: 81480.63002185506\nMSE: 10287043196.634295\nRMSE: 101425.0619750084\nR2 Square 0.9192986576295505\n__________________________________\n\n\n\n\n\n\n\n\n\nModel\nMAE\nMSE\nRMSE\nR2 Square\nCross Validation\n\n\n\n\n0\nLinear Regression\n81135.566093\n1.006842e+10\n100341.529545\n0.914682\n0.917379\n\n\n0\nRidge Regression\n81428.648355\n1.015327e+10\n100763.435337\n0.913963\n0.917379\n\n\n0\nLasso Regression\n81135.698517\n1.006845e+10\n100341.683215\n0.914682\n0.917379"
  }
]